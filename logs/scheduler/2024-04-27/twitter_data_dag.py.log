[2024-04-27T08:45:03.631+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:45:03.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:45:03.634+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:45:03.634+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:45:03.655+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:45:03.653+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:45:03.655+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:45:03.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.049 seconds
[2024-04-27T08:45:33.820+0000] {processor.py:161} INFO - Started process (PID=180) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:45:33.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:45:33.823+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:45:33.823+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:45:33.839+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:45:33.838+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:45:33.840+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:45:33.867+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.052 seconds
[2024-04-27T08:46:04.002+0000] {processor.py:161} INFO - Started process (PID=191) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:46:04.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:46:04.007+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:46:04.006+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:46:04.034+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:46:04.032+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:46:04.036+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:46:04.058+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.063 seconds
[2024-04-27T08:46:34.194+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:46:34.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:46:34.197+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:46:34.197+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:46:34.219+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:46:34.218+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:46:34.220+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:46:34.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.051 seconds
[2024-04-27T08:47:04.382+0000] {processor.py:161} INFO - Started process (PID=213) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:47:04.383+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:47:04.385+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:47:04.385+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:47:04.406+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:47:04.405+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:47:04.407+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:47:04.427+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.050 seconds
[2024-04-27T08:47:34.558+0000] {processor.py:161} INFO - Started process (PID=224) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:47:34.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:47:34.561+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:47:34.560+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:47:34.581+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:47:34.580+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:47:34.581+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:47:34.601+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.051 seconds
[2024-04-27T08:48:04.753+0000] {processor.py:161} INFO - Started process (PID=235) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:04.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:48:04.756+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:48:04.756+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:04.772+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:48:04.771+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:48:04.773+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:04.793+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.046 seconds
[2024-04-27T08:48:34.923+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:34.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:48:34.927+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:48:34.927+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:34.947+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:48:34.946+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1129, in get_code
  File "<frozen importlib._bootstrap_external>", line 1059, in source_to_code
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29
    python_callable= roberta_classifier("C:\Users\stani\projects\airflow_docker\data\all_tweets.csv")
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
[2024-04-27T08:48:34.948+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:34.970+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.054 seconds
[2024-04-27T08:48:50.121+0000] {processor.py:161} INFO - Started process (PID=257) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:50.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:48:50.124+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:48:50.123+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:54.457+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:48:54.453+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29, in <module>
    python_callable= roberta_classifier("C:/Users/stani/projects/airflow_docker/data/all_tweets.csv")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/extract_transform.py", line 29, in roberta_classifier
    tweets = pd.read_csv(file)
             ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 948, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 611, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1448, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1705, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/common.py", line 863, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/stani/projects/airflow_docker/data/all_tweets.csv'
[2024-04-27T08:48:54.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:48:54.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 4.360 seconds
[2024-04-27T08:49:16.265+0000] {processor.py:161} INFO - Started process (PID=274) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:49:16.266+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:49:16.268+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:49:16.268+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:49:19.135+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:49:19.130+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29, in <module>
    python_callable= roberta_classifier("C:/Users/stani/projects/airflow_docker/data/tweets.csv")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/extract_transform.py", line 29, in roberta_classifier
    tweets = pd.read_csv(file)
             ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 948, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 611, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1448, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1705, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/common.py", line 863, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/stani/projects/airflow_docker/data/tweets.csv'
[2024-04-27T08:49:19.137+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:49:19.158+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.899 seconds
[2024-04-27T08:49:49.379+0000] {processor.py:161} INFO - Started process (PID=303) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:49:49.381+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:49:49.382+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:49:49.382+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:49:51.833+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:49:51.828+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29, in <module>
    python_callable= roberta_classifier("C:/Users/stani/projects/airflow_docker/data/tweets.csv")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/extract_transform.py", line 29, in roberta_classifier
    tweets = pd.read_csv(file)
             ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 948, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 611, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1448, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1705, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/common.py", line 863, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/stani/projects/airflow_docker/data/tweets.csv'
[2024-04-27T08:49:51.834+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:49:51.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.482 seconds
[2024-04-27T08:50:22.025+0000] {processor.py:161} INFO - Started process (PID=326) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:50:22.026+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:50:22.028+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:22.028+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:50:24.592+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:24.583+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29, in <module>
    python_callable= roberta_classifier("C:/Users/stani/projects/airflow_docker/data/tweets.csv")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/extract_transform.py", line 29, in roberta_classifier
    tweets = pd.read_csv("C:/Users/stani/projects/airflow_docker/data/tweets.csv")
             ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 948, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 611, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1448, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1705, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/common.py", line 863, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/stani/projects/airflow_docker/data/tweets.csv'
[2024-04-27T08:50:24.593+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:50:24.610+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.591 seconds
[2024-04-27T08:50:33.711+0000] {processor.py:161} INFO - Started process (PID=343) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:50:33.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:50:33.715+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:33.715+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:50:36.549+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:50:36.776+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:36.775+0000] {override.py:1829} INFO - Created Permission View: can edit on DAG:tweets_dag_v2
[2024-04-27T08:50:36.787+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:36.786+0000] {override.py:1829} INFO - Created Permission View: can read on DAG:tweets_dag_v2
[2024-04-27T08:50:36.797+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:36.797+0000] {override.py:1829} INFO - Created Permission View: can delete on DAG:tweets_dag_v2
[2024-04-27T08:50:36.798+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:36.798+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:50:36.813+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:36.812+0000] {dag.py:3118} INFO - Creating ORM DAG for tweets_dag_v2
[2024-04-27T08:50:36.827+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:50:36.827+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-26 00:00:00+00:00, run_after=2024-04-27 00:00:00+00:00
[2024-04-27T08:50:36.850+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 3.145 seconds
[2024-04-27T08:51:07.038+0000] {processor.py:161} INFO - Started process (PID=366) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:51:07.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:51:07.041+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:51:07.041+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:51:09.455+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:51:09.477+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:51:09.476+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:51:09.498+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:51:09.497+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-26 00:00:00+00:00, run_after=2024-04-27 00:00:00+00:00
[2024-04-27T08:51:09.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.486 seconds
[2024-04-27T08:51:39.711+0000] {processor.py:161} INFO - Started process (PID=417) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:51:39.712+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:51:39.714+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:51:39.714+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:51:42.222+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:51:42.246+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:51:42.245+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:51:42.267+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:51:42.267+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:51:42.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.585 seconds
[2024-04-27T08:52:12.376+0000] {processor.py:161} INFO - Started process (PID=440) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:52:12.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:52:12.379+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:52:12.379+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:52:15.031+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:52:15.056+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:52:15.055+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:52:15.081+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:52:15.081+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:52:15.100+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.730 seconds
[2024-04-27T08:52:45.305+0000] {processor.py:161} INFO - Started process (PID=463) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:52:45.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:52:45.308+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:52:45.307+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:52:48.227+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:52:48.256+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:52:48.256+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:52:48.285+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:52:48.284+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:52:48.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 3.012 seconds
[2024-04-27T08:53:18.493+0000] {processor.py:161} INFO - Started process (PID=486) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:53:18.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:53:18.497+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:53:18.497+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:53:21.362+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:53:21.390+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:53:21.389+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:53:21.414+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:53:21.414+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:53:21.435+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.948 seconds
[2024-04-27T08:53:51.650+0000] {processor.py:161} INFO - Started process (PID=515) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:53:51.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:53:51.653+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:53:51.653+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:53:54.065+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:53:54.089+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:53:54.089+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:53:54.111+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:53:54.111+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:53:54.130+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.486 seconds
[2024-04-27T08:54:24.332+0000] {processor.py:161} INFO - Started process (PID=538) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:54:24.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:54:24.335+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:54:24.335+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:54:26.876+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:54:26.900+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:54:26.900+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:54:26.921+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:54:26.920+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:54:26.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.616 seconds
[2024-04-27T08:54:57.124+0000] {processor.py:161} INFO - Started process (PID=561) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:54:57.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:54:57.129+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:54:57.128+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:54:59.551+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:54:59.576+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:54:59.576+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:54:59.601+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:54:59.600+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:54:59.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.509 seconds
[2024-04-27T08:55:29.829+0000] {processor.py:161} INFO - Started process (PID=668) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:55:29.831+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:55:29.833+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:55:29.832+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:55:32.281+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:55:32.305+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:55:32.304+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:55:32.329+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:55:32.328+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:55:32.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.527 seconds
[2024-04-27T08:56:02.515+0000] {processor.py:161} INFO - Started process (PID=691) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:56:02.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:56:02.519+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:56:02.519+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:56:05.017+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:56:05.041+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:56:05.041+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:56:05.064+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:56:05.064+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:56:05.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.575 seconds
[2024-04-27T08:56:35.285+0000] {processor.py:161} INFO - Started process (PID=714) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:56:35.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:56:35.289+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:56:35.289+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:56:37.831+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:56:37.858+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:56:37.858+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:56:37.883+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:56:37.883+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:56:37.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.626 seconds
[2024-04-27T08:57:08.091+0000] {processor.py:161} INFO - Started process (PID=737) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:57:08.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:57:08.094+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:57:08.094+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:57:10.478+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:57:10.502+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:57:10.502+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:57:10.524+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:57:10.524+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:57:10.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.458 seconds
[2024-04-27T08:57:40.728+0000] {processor.py:161} INFO - Started process (PID=760) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:57:40.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:57:40.731+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:57:40.731+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:57:43.138+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:57:43.165+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:57:43.164+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:57:43.188+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:57:43.188+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:57:43.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.486 seconds
[2024-04-27T08:58:13.406+0000] {processor.py:161} INFO - Started process (PID=783) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:58:13.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:58:13.409+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:58:13.408+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:58:15.814+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:58:15.839+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:58:15.838+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:58:15.861+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:58:15.861+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:58:15.883+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.484 seconds
[2024-04-27T08:58:46.050+0000] {processor.py:161} INFO - Started process (PID=806) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:58:46.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:58:46.054+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:58:46.053+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:58:48.414+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:58:48.438+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:58:48.438+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:58:48.460+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:58:48.460+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:58:48.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.433 seconds
[2024-04-27T08:59:18.675+0000] {processor.py:161} INFO - Started process (PID=829) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:59:18.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:59:18.678+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:59:18.678+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:59:21.035+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:59:21.058+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:59:21.057+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:59:21.080+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:59:21.080+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:59:21.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.430 seconds
[2024-04-27T08:59:51.289+0000] {processor.py:161} INFO - Started process (PID=859) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:59:51.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T08:59:51.292+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:59:51.292+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:59:53.625+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T08:59:53.650+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:59:53.649+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T08:59:53.671+0000] {logging_mixin.py:188} INFO - [2024-04-27T08:59:53.671+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T08:59:53.690+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.407 seconds
[2024-04-27T09:00:23.893+0000] {processor.py:161} INFO - Started process (PID=924) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:00:23.894+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:00:23.898+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:00:23.897+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:00:26.435+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:00:26.458+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:00:26.457+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:00:26.478+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:00:26.478+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:00:26.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.613 seconds
[2024-04-27T09:00:56.709+0000] {processor.py:161} INFO - Started process (PID=947) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:00:56.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:00:56.712+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:00:56.712+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:00:59.150+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:00:59.176+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:00:59.175+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:00:59.205+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:00:59.205+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:00:59.236+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.533 seconds
[2024-04-27T09:01:29.449+0000] {processor.py:161} INFO - Started process (PID=1082) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:01:29.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:01:29.453+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:01:29.452+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:01:31.942+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:01:31.967+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:01:31.966+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:01:31.992+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:01:31.992+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:01:32.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.569 seconds
[2024-04-27T09:02:02.215+0000] {processor.py:161} INFO - Started process (PID=1105) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:02:02.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:02:02.218+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:02:02.218+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:02:04.656+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:02:04.686+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:02:04.685+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:02:04.717+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:02:04.716+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:02:04.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.529 seconds
[2024-04-27T09:02:34.924+0000] {processor.py:161} INFO - Started process (PID=1269) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:02:34.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:02:34.928+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:02:34.927+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:02:37.518+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:02:37.549+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:02:37.548+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:02:37.575+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:02:37.575+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:02:37.600+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.683 seconds
[2024-04-27T09:03:07.819+0000] {processor.py:161} INFO - Started process (PID=1292) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:07.820+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:03:07.822+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:07.822+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:10.309+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:10.331+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:10.331+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:03:10.352+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:10.352+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:03:10.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.560 seconds
[2024-04-27T09:03:40.569+0000] {processor.py:161} INFO - Started process (PID=1315) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:40.570+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:03:40.572+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:40.572+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:43.021+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:43.045+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:43.045+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:03:43.069+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:43.069+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:03:43.091+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.529 seconds
[2024-04-27T09:03:55.643+0000] {processor.py:161} INFO - Started process (PID=1338) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:55.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:03:55.646+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:55.646+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:58.187+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:03:58.182+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29, in <module>
    python_callable= roberta_classifier("./data/tweets.csv")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/extract_transform.py", line 29, in roberta_classifier
    tweets = pd.read_csv(file)
             ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 948, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 611, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1448, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1705, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/common.py", line 863, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './data/tweets.csv'
[2024-04-27T09:03:58.189+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:03:58.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.569 seconds
[2024-04-27T09:04:28.391+0000] {processor.py:161} INFO - Started process (PID=1361) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:04:28.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:04:28.395+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:04:28.394+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:04:30.915+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:04:30.910+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 29, in <module>
    python_callable= roberta_classifier("./data/tweets.csv")
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/scripts/extract_transform.py", line 29, in roberta_classifier
    tweets = pd.read_csv(file)
             ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 948, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 611, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1448, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py", line 1705, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/io/common.py", line 863, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './data/tweets.csv'
[2024-04-27T09:04:30.917+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:04:30.934+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.549 seconds
[2024-04-27T09:05:01.138+0000] {processor.py:161} INFO - Started process (PID=1384) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:01.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:05:01.143+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:01.142+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:01.181+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:01.177+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 6, in <module>
    from scripts.extract_transform import  roberta_classifier
ModuleNotFoundError: No module named 'scripts.extract_transform'
[2024-04-27T09:05:01.183+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:01.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.077 seconds
[2024-04-27T09:05:07.279+0000] {processor.py:161} INFO - Started process (PID=1389) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:07.280+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:05:07.283+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:07.283+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:07.335+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:07.330+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 6, in <module>
    from scripts.extract_transform import  roberta_classifier
ModuleNotFoundError: No module named 'scripts.extract_transform'
[2024-04-27T09:05:07.336+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:07.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.085 seconds
[2024-04-27T09:05:08.303+0000] {processor.py:161} INFO - Started process (PID=1394) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:08.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:05:08.307+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:08.307+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:08.362+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:08.358+0000] {dagbag.py:350} ERROR - Failed to import: /opt/airflow/dags/twitter_data_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py", line 346, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/opt/airflow/dags/twitter_data_dag.py", line 6, in <module>
    from scripts.extract_transform import  roberta_classifier
ModuleNotFoundError: No module named 'scripts.extract_transform'
[2024-04-27T09:05:08.365+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:08.386+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 0.091 seconds
[2024-04-27T09:05:23.510+0000] {processor.py:161} INFO - Started process (PID=1405) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:23.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:05:23.513+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:23.513+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:26.908+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:27.028+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:27.027+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:05:27.048+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:27.048+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:05:27.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 3.571 seconds
[2024-04-27T09:05:57.299+0000] {processor.py:161} INFO - Started process (PID=1568) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:57.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:05:57.302+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:57.302+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:59.827+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:05:59.851+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:59.851+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:05:59.873+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:05:59.872+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:05:59.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.601 seconds
[2024-04-27T09:06:30.082+0000] {processor.py:161} INFO - Started process (PID=1591) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:06:30.084+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:06:30.085+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:06:30.084+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:06:32.402+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:06:32.427+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:06:32.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:06:32.448+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:06:32.448+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:06:32.468+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.392 seconds
[2024-04-27T09:07:02.669+0000] {processor.py:161} INFO - Started process (PID=1614) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:07:02.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:07:02.672+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:07:02.671+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:07:05.170+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:07:05.197+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:07:05.197+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:07:05.220+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:07:05.219+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:07:05.238+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.584 seconds
[2024-04-27T09:07:34.479+0000] {processor.py:161} INFO - Started process (PID=1637) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:07:34.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:07:34.482+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:07:34.482+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:07:37.427+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:07:37.450+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:07:37.450+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:07:37.476+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:07:37.476+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:07:37.502+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 3.037 seconds
[2024-04-27T09:08:07.668+0000] {processor.py:161} INFO - Started process (PID=1829) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:08:07.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:08:07.671+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:08:07.671+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:08:10.214+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:08:10.238+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:08:10.238+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:08:10.264+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:08:10.264+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:08:10.284+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.623 seconds
[2024-04-27T09:08:40.520+0000] {processor.py:161} INFO - Started process (PID=2030) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:08:40.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:08:40.540+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:08:40.539+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:08:43.795+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:08:43.824+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:08:43.823+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:08:43.852+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:08:43.852+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:08:43.873+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 3.366 seconds
[2024-04-27T09:09:14.047+0000] {processor.py:161} INFO - Started process (PID=2071) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:09:14.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:09:14.050+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:09:14.049+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:09:16.523+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:09:16.549+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:09:16.548+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:09:16.574+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:09:16.573+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:09:16.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.555 seconds
[2024-04-27T09:09:46.803+0000] {processor.py:161} INFO - Started process (PID=2108) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:09:46.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:09:46.805+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:09:46.805+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:09:52.886+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:09:52.969+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:09:52.967+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:09:53.043+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:09:53.043+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:09:53.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 6.296 seconds
[2024-04-27T09:10:23.229+0000] {processor.py:161} INFO - Started process (PID=2367) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:10:23.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:10:23.238+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:10:23.236+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:10:29.720+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:10:29.796+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:10:29.795+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:10:29.860+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:10:29.859+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:10:29.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 6.707 seconds
[2024-04-27T09:10:57.915+0000] {processor.py:161} INFO - Started process (PID=2502) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:10:57.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:10:57.919+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:10:57.918+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:07.610+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:07.653+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:07.652+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T09:01:13.789783+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:11:07.681+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:07.681+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T090113, start_date=20240427T090959, end_date=20240427T091107
[2024-04-27T09:11:07.705+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T09:01:13.789783+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:11:07.752+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:07.751+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:11:07.784+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:07.784+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:11:07.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 9.908 seconds
[2024-04-27T09:11:08.112+0000] {processor.py:161} INFO - Started process (PID=2532) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:08.116+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:11:08.118+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:08.117+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:15.842+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:15.887+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:15.887+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T08:51:15.232230+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:11:15.914+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:15.913+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T085115, start_date=20240427T090959, end_date=20240427T091115
[2024-04-27T09:11:15.930+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T08:51:15.232230+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:11:15.937+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:15.936+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T09:02:15.515320+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:11:15.951+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:15.950+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T090215, start_date=20240427T090959, end_date=20240427T091115
[2024-04-27T09:11:15.960+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T09:02:15.515320+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:11:15.966+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:15.965+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T08:51:15.232230+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:11:15.979+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:15.978+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T085115, start_date=20240427T090959, end_date=20240427T091115
[2024-04-27T09:11:15.992+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T08:51:15.232230+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:11:16.025+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:16.024+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:11:16.066+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:16.066+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:11:16.115+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 8.024 seconds
[2024-04-27T09:11:46.504+0000] {processor.py:161} INFO - Started process (PID=2571) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:46.507+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:11:46.512+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:46.511+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:52.521+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:11:52.560+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:52.559+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T09:02:15.515320+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:11:52.586+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:52.585+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T090215, start_date=20240427T090959, end_date=20240427T091152
[2024-04-27T09:11:52.600+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T09:02:15.515320+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:11:52.630+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:52.629+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:11:52.667+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:11:52.667+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:11:52.694+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 6.200 seconds
[2024-04-27T09:12:22.965+0000] {processor.py:161} INFO - Started process (PID=2617) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:12:22.966+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:12:22.968+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:12:22.967+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:12:25.419+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:12:25.442+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:12:25.441+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:12:25.463+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:12:25.463+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:12:25.487+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.529 seconds
[2024-04-27T09:12:55.641+0000] {processor.py:161} INFO - Started process (PID=2657) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:12:55.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:12:55.644+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:12:55.644+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:12:58.745+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:12:58.774+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:12:58.774+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:12:58.799+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:12:58.798+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:12:58.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 3.187 seconds
[2024-04-27T09:13:29.066+0000] {processor.py:161} INFO - Started process (PID=2697) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:13:29.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:13:29.069+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:13:29.068+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:13:32.026+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:13:32.056+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:13:32.055+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:13:32.085+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:13:32.085+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:13:32.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 3.055 seconds
[2024-04-27T09:14:02.341+0000] {processor.py:161} INFO - Started process (PID=2737) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:14:02.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:14:02.343+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:14:02.343+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:14:04.966+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:14:04.990+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:14:04.990+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:14:05.012+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:14:05.012+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:14:05.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.696 seconds
[2024-04-27T09:14:35.259+0000] {processor.py:161} INFO - Started process (PID=2777) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:14:35.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:14:35.261+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:14:35.261+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:14:37.614+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:14:37.638+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:14:37.637+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:14:37.659+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:14:37.659+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:14:37.680+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.428 seconds
[2024-04-27T09:15:07.966+0000] {processor.py:161} INFO - Started process (PID=2958) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:07.968+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:15:07.970+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:07.970+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:12.482+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:12.530+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:12.529+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:15:12.576+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:12.575+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:15:12.605+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 4.653 seconds
[2024-04-27T09:15:32.861+0000] {processor.py:161} INFO - Started process (PID=3189) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:32.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:15:32.865+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:32.864+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:40.316+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:40.352+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:40.351+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T09:02:15.515320+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:15:40.390+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:40.389+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T090215, start_date=20240427T091509, end_date=20240427T091540
[2024-04-27T09:15:40.407+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T09:02:15.515320+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:15:40.542+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:40.542+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:15:40.600+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:40.599+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:15:40.682+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 7.835 seconds
[2024-04-27T09:15:43.085+0000] {processor.py:161} INFO - Started process (PID=3219) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:43.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:15:43.091+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:43.090+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:49.906+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:15:49.957+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:49.957+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T08:51:15.232230+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:15:49.982+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:49.982+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T085115, start_date=20240427T091509, end_date=20240427T091549
[2024-04-27T09:15:49.994+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T08:51:15.232230+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:15:49.999+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:49.998+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T09:01:13.789783+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:15:50.008+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:50.008+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T090113, start_date=20240427T091509, end_date=20240427T091550
[2024-04-27T09:15:50.013+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T09:01:13.789783+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:15:50.017+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:50.017+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T08:51:15.232230+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:15:50.027+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:50.026+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T085115, start_date=20240427T091509, end_date=20240427T091550
[2024-04-27T09:15:50.032+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T08:51:15.232230+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:15:50.061+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:50.060+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:15:50.093+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:15:50.093+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:15:50.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 7.059 seconds
[2024-04-27T09:16:20.318+0000] {processor.py:161} INFO - Started process (PID=3259) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:16:20.343+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:16:20.347+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:20.346+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:16:26.078+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:16:26.134+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:26.133+0000] {taskinstance.py:2892} ERROR - {'DAG Id': 'tweets_dag_v2', 'Task Id': 'execute_python_script', 'Run Id': 'manual__2024-04-27T09:01:13.789783+00:00', 'Hostname': 'cd57c070245a'}
[2024-04-27T09:16:26.164+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:26.163+0000] {taskinstance.py:1205} INFO - Marking task as UP_FOR_RETRY. dag_id=tweets_dag_v2, task_id=execute_python_script, execution_date=20240427T090113, start_date=20240427T091509, end_date=20240427T091626
[2024-04-27T09:16:26.178+0000] {processor.py:791} INFO - Executed failure callback for <TaskInstance: tweets_dag_v2.execute_python_script manual__2024-04-27T09:01:13.789783+00:00 [up_for_retry]> in state up_for_retry
[2024-04-27T09:16:26.209+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:26.208+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:16:26.244+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:26.243+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:16:26.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 5.981 seconds
[2024-04-27T09:16:56.569+0000] {processor.py:161} INFO - Started process (PID=3305) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:16:56.570+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:16:56.572+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:56.571+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:16:59.311+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:16:59.336+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:59.336+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:16:59.361+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:16:59.361+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:16:59.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.822 seconds
[2024-04-27T09:17:29.635+0000] {processor.py:161} INFO - Started process (PID=3345) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:17:29.636+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:17:29.637+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:17:29.637+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:17:32.129+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:17:32.153+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:17:32.153+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:17:32.176+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:17:32.176+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:17:32.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.571 seconds
[2024-04-27T09:18:02.422+0000] {processor.py:161} INFO - Started process (PID=3386) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:18:02.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:18:02.425+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:18:02.424+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:18:04.926+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:18:04.951+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:18:04.950+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:18:04.973+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:18:04.973+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:18:04.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.579 seconds
[2024-04-27T09:18:35.235+0000] {processor.py:161} INFO - Started process (PID=3426) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:18:35.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:18:35.237+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:18:35.237+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:18:37.873+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:18:37.896+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:18:37.896+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:18:37.919+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:18:37.919+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:18:37.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.712 seconds
[2024-04-27T09:19:08.168+0000] {processor.py:161} INFO - Started process (PID=3466) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:19:08.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:19:08.170+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:19:08.170+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:19:10.476+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:19:10.503+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:19:10.503+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:19:10.533+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:19:10.532+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:19:10.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.400 seconds
[2024-04-27T09:19:40.808+0000] {processor.py:161} INFO - Started process (PID=3506) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:19:40.809+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:19:40.810+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:19:40.810+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:19:43.260+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:19:43.287+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:19:43.287+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:19:43.313+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:19:43.313+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:19:43.339+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.542 seconds
[2024-04-27T09:20:13.539+0000] {processor.py:161} INFO - Started process (PID=3561) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:20:13.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:20:13.542+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:20:13.542+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:20:15.953+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:20:15.974+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:20:15.974+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:20:15.995+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:20:15.995+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:20:16.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.482 seconds
[2024-04-27T09:20:46.286+0000] {processor.py:161} INFO - Started process (PID=3615) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:20:46.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:20:46.289+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:20:46.289+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:20:49.032+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:20:49.069+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:20:49.068+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:20:49.109+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:20:49.109+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:20:49.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.873 seconds
[2024-04-27T09:21:19.536+0000] {processor.py:161} INFO - Started process (PID=3707) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:21:19.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:21:19.538+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:21:19.538+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:21:22.228+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:21:22.252+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:21:22.252+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:21:22.275+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:21:22.274+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:21:22.296+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.768 seconds
[2024-04-27T09:21:52.540+0000] {processor.py:161} INFO - Started process (PID=3779) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:21:52.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:21:52.542+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:21:52.542+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:21:54.922+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:21:54.943+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:21:54.943+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:21:54.965+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:21:54.964+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:21:54.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.451 seconds
[2024-04-27T09:22:25.226+0000] {processor.py:161} INFO - Started process (PID=3825) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:22:25.227+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:22:25.228+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:22:25.228+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:22:27.552+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:22:27.576+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:22:27.576+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:22:27.597+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:22:27.597+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:22:27.614+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.395 seconds
[2024-04-27T09:22:57.862+0000] {processor.py:161} INFO - Started process (PID=3865) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:22:57.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:22:57.864+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:22:57.864+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:23:00.212+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:23:00.235+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:23:00.235+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:23:00.256+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:23:00.256+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:23:00.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.422 seconds
[2024-04-27T09:23:30.458+0000] {processor.py:161} INFO - Started process (PID=3905) to work on /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:23:30.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/twitter_data_dag.py for tasks to queue
[2024-04-27T09:23:30.460+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:23:30.460+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:23:32.976+0000] {processor.py:840} INFO - DAG(s) 'tweets_dag_v2' retrieved from /opt/airflow/dags/twitter_data_dag.py
[2024-04-27T09:23:33.002+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:23:33.002+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2024-04-27T09:23:33.025+0000] {logging_mixin.py:188} INFO - [2024-04-27T09:23:33.025+0000] {dag.py:3954} INFO - Setting next_dagrun for tweets_dag_v2 to 2024-04-27 00:00:00+00:00, run_after=2024-04-28 00:00:00+00:00
[2024-04-27T09:23:33.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/twitter_data_dag.py took 2.594 seconds
